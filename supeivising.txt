_______________ notes about the models performance _____________________________________________________
model = Sequential()
model.add(LSTM(500, return_sequences=True, input_shape=(x_train.shape[1], 5)))
model.add(Dropout(0.2))
model.add(LSTM(50, return_sequences=True)) # Add another LSTM layer
model.add(Dropout(0.2))
model.add(LSTM(50, return_sequences=False))

model.add(Dense(20))

name of the model ---> hello2.h5


_________________________________________________________________________________________
model 2 ---> the8features_model.h5 
it takes the last 60 values of 'Open', 'High', 'Low', 'volume','spread', 'Close'
retrained with ---> data\\EURUSD_p3.csv
Epoch 5/5
12315/12315 [==============================] - 3726s 303ms/step - loss: 8.8096e-05
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

another retraiin with ---> data\\EURUSD_p4.csv
Epoch 1/5
12315/12315 [==============================] - 3983s 323ms/step - loss: 6.5774e-05
Epoch 2/5
12315/12315 [==============================] - 4412s 358ms/step - loss: 6.2041e-05
Epoch 3/5
12315/12315 [==============================] - 4131s 335ms/step - loss: 5.6475e-05
Epoch 4/5
12315/12315 [==============================] - 4166s 338ms/step - loss: 4.8213e-05
Epoch 5/5
12315/12315 [==============================] - 4632s 376ms/step - loss: 4.9030e-05
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

another train with ----> data = pd.read_csv('data\\EURUSD_p5.csv')
Epoch 1/5
12315/12315 [==============================] - 4131s 335ms/step - loss: 6.9627e-05
Epoch 2/5
12315/12315 [==============================] - 3631s 295ms/step - loss: 6.2552e-05
Epoch 3/5
12315/12315 [==============================] - 3767s 306ms/step - loss: 5.6979e-05
Epoch 4/5
12315/12315 [==============================] - 3644s 296ms/step - loss: 5.4944e-05
Epoch 5/5
12315/12315 [==============================] - 3752s 305ms/step - loss: 5.2906e-05
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

another train --> with data = pd.read_csv('data\\EURUSD_p5.csv')
----> the saved model is the8features_model2.h5

Epoch 1/10
12315/12315 [==============================] - 3765s 305ms/step - loss: 5.2065e-05
Epoch 2/10
12315/12315 [==============================] - 3696s 300ms/step - loss: 4.7669e-05
Epoch 3/10
12315/12315 [==============================] - 3677s 299ms/step - loss: 4.5405e-05
Epoch 4/10
12315/12315 [==============================] - 3654s 297ms/step - loss: 4.5336e-05
Epoch 5/10
12315/12315 [==============================] - 3665s 298ms/step - loss: 4.5192e-05
Epoch 6/10
12315/12315 [==============================] - 3651s 296ms/step - loss: 4.4626e-05
Epoch 7/10
12315/12315 [==============================] - 3659s 297ms/step - loss: 4.3040e-05
Epoch 8/10
12315/12315 [==============================] - 3671s 298ms/step - loss: 4.0616e-05
Epoch 9/10
12315/12315 [==============================] - 3663s 297ms/step - loss: 4.0579e-05
Epoch 10/10
12315/12315 [==============================] - 3664s 297ms/step - loss: 3.9059e-05

---> another train of model = load_model('the8features_model2.h5')
---> with data = pd.read_csv('data\\EURUSD_p7.csv')

model.fit(x_train, y_train, batch_size=1, epochs=5)
model.save('the8features_model3.h5')
Epoch 5/5
loss [================================] loss: 5.7226e-05

--> another train with -->data = pd.read_csv('data\\EURUSD_p8.csv')
model = load_model('the8features_model3.h5')
model.fit(x_train, y_train, batch_size=1, epochs=8)
model.save('the8features_model4.h5')
Epoch 1/8
5440/5440 [==============================] - 1973s 360ms/step - loss: 1.3874e-04
Epoch 2/8
5440/5440 [==============================] - 1940s 357ms/step - loss: 1.2688e-04
Epoch 3/8
5440/5440 [==============================] - 1942s 357ms/step - loss: 1.2306e-04
Epoch 4/8
5440/5440 [==============================] - 1942s 357ms/step - loss: 1.2299e-04
Epoch 5/8
5440/5440 [==============================] - 1937s 356ms/step - loss: 1.2192e-04
Epoch 6/8
5440/5440 [==============================] - 1944s 357ms/step - loss: 1.2154e-04
Epoch 7/8
5440/5440 [==============================] - 1950s 358ms/step - loss: 1.1914e-04
Epoch 8/8
5440/5440 [==============================] - 1953s 359ms/step - loss: 1.1827e-04

--> another train with -->data = pd.read_csv('data\\EURUSD_p8.csv') but this time 3 epochs
model = load_model('the8features_model4.h5')
model.fit(x_train, y_train, batch_size=1, epochs=3)
model.save('the8features_model5.h5')

Epoch 1/3
5440/5440 [==============================] - 2041s 375ms/step - loss: 1.1555e-04
Epoch 2/3
5440/5440 [==============================] - 1986s 365ms/step - loss: 1.1218e-04
Epoch 3/3
5440/5440 [==============================] - 1990s 366ms/step - loss: 1.1244e-04



--> another retrain with -- > data = pd.read_csv('data\\EURUSD_p9.csv') # i added also the validation_loss beside the train loss 

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)

history = model.fit(x_train, y_train, batch_size=1, epochs=8, validation_data=(x_val, y_val))
model.save('the8features_model7.h5')

Epoch 1/8
4352/4352 [==============================] - 2426s 556ms/step - loss: 3.2029e-05 - val_loss: 3.1129e-05
Epoch 2/8
4352/4352 [==============================] - 1611s 370ms/step - loss: 3.2873e-05 - val_loss: 3.9331e-05
Epoch 3/8
4352/4352 [==============================] - 4567s 1s/step - loss: 3.1153e-05 - val_loss: 7.4028e-05
Epoch 4/8
4352/4352 [==============================] - 2268s 521ms/step - loss: 3.4435e-05 - val_loss: 4.2236e-05
Epoch 5/8
4352/4352 [==============================] - 1576s 362ms/step - loss: 3.0511e-05 - val_loss: 6.3023e-05
Epoch 6/8
4352/4352 [==============================] - 3413s 784ms/step - loss: 3.1306e-05 - val_loss: 1.8366e-04
Epoch 7/8
4352/4352 [==============================] - 1588s 365ms/step - loss: 3.2245e-05 - val_loss: 4.1872e-05
Epoch 8/8
4352/4352 [==============================] - 1583s 364ms/step - loss: 3.4488e-05 - val_loss: 3.5329e-05


--> another retrain with data = pd.read_csv('data\\EURUSD_p11.csv')
model = load_model('the8features_model8.h5')

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)


history = model.fit(x_train, y_train, batch_size=1, epochs=2, validation_data=(x_val, y_val))
model.save('the8features_model9.h5')

Epoch 1/2
4352/4352 [==============================] - 1630s 374ms/step - loss: 1.1379e-04 - val_loss: 1.1036e-04
Epoch 2/2
4352/4352 [==============================] - ETA: 0s - loss: 1.0251e-04

note --> i found that the best nuber of epochs is 2 in this train after that i saw overfitting since the third epoch 

---> another retrain with data = pd.read_csv('data\\EURUSD_p12.csv')
model = load_model('the8features_model9.h5')


x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)

history = model.fit(x_train, y_train, batch_size=1, epochs=11, validation_data=(x_val, y_val))
model.save('the8features_model10.h5')

Epoch 1/11
4352/4352 [==============================] - 1711s 392ms/step - loss: 1.3547e-04 - val_loss: 1.1516e-04
Epoch 2/11
4352/4352 [==============================] - 1591s 366ms/step - loss: 1.3067e-04 - val_loss: 1.1862e-04
Epoch 3/11
4352/4352 [==============================] - 1592s 366ms/step - loss: 1.2939e-04 - val_loss: 1.2388e-04
Epoch 4/11
4352/4352 [==============================] - 1592s 366ms/step - loss: 1.2627e-04 - val_loss: 1.3607e-04
Epoch 5/11
4352/4352 [==============================] - 1595s 367ms/step - loss: 1.2179e-04 - val_loss: 1.1417e-04
Epoch 6/11
4352/4352 [==============================] - 1669s 384ms/step - loss: 1.2212e-04 - val_loss: 1.1095e-04
Epoch 7/11
4352/4352 [==============================] - 1589s 365ms/step - loss: 1.2151e-04 - val_loss: 9.6770e-05
Epoch 8/11
4352/4352 [==============================] - 1586s 364ms/step - loss: 1.2229e-04 - val_loss: 9.2301e-05
Epoch 9/11
4352/4352 [==============================] - 1588s 365ms/step - loss: 1.2177e-04 - val_loss: 1.0516e-04
Epoch 10/11
4352/4352 [==============================] - 1588s 365ms/step - loss: 1.2066e-04 - val_loss: 9.5786e-05
Epoch 11/11
4352/4352 [==============================] - 1588s 365ms/step - loss: 1.2030e-04 - val_loss: 9.7618e-05

--> another train with  data = pd.read_csv('data\\EURUSD_p13.csv')
model = load_model('the8features_model10.h5')

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)
history = model.fit(x_train, y_train, batch_size=1, epochs=2, validation_data=(x_val, y_val))
model.save('the8features_model11.h5')

Epoch 1/2
4352/4352 [==============================] - 1572s 360ms/step - loss: 1.2877e-04 - val_loss: 7.3016e-05
Epoch 2/2
4352/4352 [==============================] - 1493s 343ms/step - loss: 1.2784e-04 - val_loss: 6.8626e-05

--> another train with  data = pd.read_csv('data\\EURUSD_p14.csv')

model = load_model('the8features_model11.h5')

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)


history = model.fit(x_train, y_train, batch_size=1, epochs=7, validation_data=(x_val, y_val))
model.save('the8features_model12.h5')

______________________tuns out that 2 epochs are better so the train is bellow
model = load_model('the8features_model12.h5')

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)


history = model.fit(x_train, y_train, batch_size=1, epochs=2, validation_data=(x_val, y_val))
model.save('the8features_model13.h5')

Epoch 1/2
4352/4352 [==============================] - 1616s 371ms/step - loss: 5.4172e-05 - val_loss: 5.3073e-05
Epoch 2/2
4352/4352 [==============================] - ETA: 0s - loss: 5.4841e-05








late on i will be testing new models and see what is the best one what gives the best loss in short time 
i want 


(ml) C:\Users\abdel\Desktop\venvs\ml\root\models>py Bidirectional.py
(5480, 20, 3)
(5480, 20, 1)
2023-12-19 19:29:57.740192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/5
548/548 [==============================] - 310s 492ms/step - loss: 0.0065
Epoch 2/5
548/548 [==============================] - 481s 877ms/step - loss: 0.0021
Epoch 3/5
548/548 [==============================] - 411s 750ms/step - loss: 0.0014
Epoch 4/5
548/548 [==============================] - 371s 678ms/step - loss: 0.0011
Epoch 5/5
548/548 [==============================] - 190s 346ms/step - loss: 8.8178e-04


(ml) C:\Users\abdel\Desktop\venvs\ml\root\models>py tl.py      
(5480, 20, 3)
(5480, 20, 1)
2023-12-19 19:35:04.780331: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/5
548/548 [==============================] - 343s 478ms/step - loss: 0.0040
Epoch 2/5
 89/548 [===>..........................] - ETA: 3:15 - loss: 4.9166e-04 90/548 [===>..........................] - ETA: 3:14 - loss: 4.9481e-04548/548 [==============================] - 221s 403ms/step - loss: 4.4572e-04
Epoch 3/5
548/548 [==============================] - 235s 429ms/step - loss: 3.5502e-04
Epoch 4/5
548/548 [==============================] - 207s 377ms/step - loss: 2.8221e-04
Epoch 5/5
548/548 [==============================] - 215s 393ms/step - loss: 2.5157e-04



(ml) C:\Users\abdel\Desktop\venvs\ml\root\models>py tl.py
(5480, 20, 5)
(5480, 20, 1)
2023-12-20 11:09:45.594972: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/5
5480/5480 [==============================] - 1846s 330ms/step - loss: 0.0014
Epoch 2/5
5480/5480 [==============================] - 1620s 296ms/step - loss: 6.5374e-04
Epoch 3/5
5480/5480 [==============================] - 1536s 280ms/step - loss: 4.9209e-04
Epoch 4/5
5480/5480 [==============================] - 1531s 279ms/step - loss: 4.4206e-04
Epoch 5/5
5480/5480 [==============================] - 1534s 280ms/step - loss: 3.1794e-04

(ml) C:\Users\abdel\Desktop\venvs\ml\root\models>py tl.py
(5480, 20, 5)
(5480, 20, 1)
2023-12-20 13:53:21.750602: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/5
5480/5480 [==============================] - 1589s 281ms/step - loss: 0.0022
Epoch 2/5
5480/5480 [==============================] - 786s 143ms/step - loss: 9.5923e-04
Epoch 3/5
5480/5480 [==============================] - 696s 127ms/step - loss: 6.2628e-04
Epoch 4/5
5480/5480 [==============================] - 693s 126ms/step - loss: 5.0290e-04
Epoch 5/5
5480/5480 [==============================] - 696s 127ms/step - loss: 4.2151e-04
--> saved as tl.h5 

retrained hello3.h5 
Epoch 1/5
5480/5480 [==============================] - 2632s 479ms/step - loss: 3.8025e-04
Epoch 2/5
5480/5480 [==============================] - 2685s 490ms/step - loss: 2.9415e-04
Epoch 3/5
5480/5480 [==============================] - 2641s 482ms/step - loss: 2.7674e-04
Epoch 4/5
5480/5480 [==============================] - 1830s 334ms/step - loss: 2.7667e-04
Epoch 5/5
5480/5480 [==============================] - 2321s 424ms/step - loss: 2.4087e-04
---> saved it as hello4.h5

